{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aws(0420)_cat_dog_small_data_CNN 구현 및 문제점 해결.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNAVPMIVJK7RIiChrE2epgK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljy9969/Study/blob/master/aws(0420)_cat_dog_small_data_CNN_%EA%B5%AC%ED%98%84_%EB%B0%8F_%EB%AC%B8%EC%A0%9C%EC%A0%90_%ED%95%B4%EA%B2%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Atx-TvnaU6xX"
      },
      "outputs": [],
      "source": [
        "# 일부 이미지 분리(총 4000개)\n",
        "import os, shutil\n",
        "\n",
        "original_dataset_dir = './data/cat_dog/train'\n",
        "\n",
        "## directory 생성 ##\n",
        "\n",
        "base_dir = 'data/cat_dog_small'\n",
        "os.mkdir(base_dir)\n",
        "\n",
        "train_dir = os.path.join(base_dir,'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir,'validation')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir,'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir,'cats')\n",
        "os.mkdir(train_cats_dir)\n",
        "train_dogs_dir = os.path.join(train_dir,'dogs')\n",
        "os.mkdir(train_dogs_dir)\n",
        "\n",
        "validation_cats_dir = os.path.join(validation_dir,'cats')\n",
        "os.mkdir(validation_cats_dir)\n",
        "validation_dogs_dir = os.path.join(validation_dir,'dogs')\n",
        "os.mkdir(validation_dogs_dir)\n",
        "\n",
        "test_cats_dir = os.path.join(test_dir,'cats')\n",
        "os.mkdir(test_cats_dir)\n",
        "test_dogs_dir = os.path.join(test_dir,'dogs')\n",
        "os.mkdir(test_dogs_dir)\n",
        "\n",
        "## file 복사 ##\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir,fname)\n",
        "    dst = os.path.join(train_cats_dir, fname)\n",
        "    shutil.copyfile(src,dst)\n",
        "\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1000,1500)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir,fname)\n",
        "    dst = os.path.join(validation_cats_dir, fname)\n",
        "    shutil.copyfile(src,dst)\n",
        "    \n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1500,2000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir,fname)\n",
        "    dst = os.path.join(test_cats_dir, fname)\n",
        "    shutil.copyfile(src,dst)\n",
        "    \n",
        "\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir,fname)\n",
        "    dst = os.path.join(train_dogs_dir, fname)\n",
        "    shutil.copyfile(src,dst)\n",
        "\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1000,1500)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir,fname)\n",
        "    dst = os.path.join(validation_dogs_dir, fname)\n",
        "    shutil.copyfile(src,dst)\n",
        "    \n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1500,2000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir,fname)\n",
        "    dst = os.path.join(test_dogs_dir, fname)\n",
        "    shutil.copyfile(src,dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 이미지 증식(Image Augmentation)"
      ],
      "metadata": {
        "id": "iSqho2wqVij4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keras가 제공하는 ImageDataGenerator\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_dir = './data/cat_dog_small/train'\n",
        "valid_dir = './data/cat_dog_small/validation'\n",
        "\n",
        "# ImageDataGenerator 생성\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,               # target directory\n",
        "    classes=['cats','dogs'], # [0, 1]\n",
        "    target_size=(150,150),   # image resize\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    valid_dir,               # target directory\n",
        "    classes=['cats','dogs'], # [0, 1]\n",
        "    target_size=(150,150),   # image resize\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "id": "lQAp5xbkVSwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam # or RMSprop\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=32,\n",
        "                 kernel_size=(3,3),\n",
        "                 activation='relu',\n",
        "                 input_shape=(150,150,3))) # color!\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=64,\n",
        "                 kernel_size=(3,3),\n",
        "                 activation='relu'))\n",
        "\n",
        "model.add(Conv2D(filters=128,\n",
        "                 kernel_size=(3,3),\n",
        "                 activation='relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=128,\n",
        "                 kernel_size=(3,3),\n",
        "                 activation='relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# model.add(Dropout(rate=0.5))\n",
        "\n",
        "model.add(Dense(units=256, activation='relu'))\n",
        "\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "FQcG_7TzVU48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator, # 이미지 2000개 / 20개씩 batch = 100번\n",
        "                   steps_per_epoch=100,\n",
        "                   epochs=30,\n",
        "                   validation_data=validation_generator, # 이미지 1000개 / 20개씩 batch = 50번\n",
        "                   validation_steps=50)\n",
        "\n",
        "model.save('./data/cats_dogs_small_cnn_model.h5')"
      ],
      "metadata": {
        "id": "KI9yttqjVVoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history 객체로 그래프 그려보기\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_acc = history.history['accuracy']\n",
        "valid_acc = history.history['val_accuracy']\n",
        "\n",
        "train_loss = history.history['loss']\n",
        "valid_loss = history.history['val_loss']\n",
        "\n",
        "figure = plt.figure(figsize=(20,5))\n",
        "ax1 = figure.add_subplot(1,2,1)\n",
        "ax2 = figure.add_subplot(1,2,2)\n",
        "\n",
        "ax1.plot(train_acc, color='r', label='train accuracy')\n",
        "ax1.plot(valid_acc, color='b', label='valid accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid()\n",
        "\n",
        "ax2.plot(train_loss, color='r', label='train loss')\n",
        "ax2.plot(valid_loss, color='b', label='valid loss')\n",
        "ax2.legend()\n",
        "ax2.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w3qM2940VVq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Augmentation(이미지 증식)\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# train datagen = ImageDataGenerator(rescale=1/255)\n",
        "datagen = ImageDataGenerator(rotation_range=20,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=0.1,\n",
        "                             horizontal_flip=True,\n",
        "                             vertical_flip=True,\n",
        "                             fill_mode='nearest') # 보정\n",
        "\n",
        "img = image.load_img('./data/cat_dog_small/train/cats/cat.3.jpg',\n",
        "                    target_size=(150,150))\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "print(type(x), x.shape)\n",
        "\n",
        "x = x.reshape((1,) + x.shape)\n",
        "print(x.shape)\n",
        "\n",
        "figure = plt.figure()\n",
        "ax = []\n",
        "\n",
        "for i in range(20):\n",
        "    ax.append(figure.add_subplot(4,5,i+1))\n",
        "    \n",
        "idx = 0\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "    ax[idx].imshow(image.array_to_img(batch[0]))\n",
        "    idx += 1\n",
        "    if idx == 20:\n",
        "        break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oPmGadBIVVtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 증식을 이용해서 4000개의 이미지를 학습\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_dir = './data/cat_dog_small/train'\n",
        "valid_dir = './data/cat_dog_small/validation'\n",
        "\n",
        "# ImageDataGenerator 생성\n",
        "train_datagen = ImageDataGenerator(rescale=1/255,\n",
        "                                   rotation_range=30, # ImageDataGenerator + Image Augmentation\n",
        "                                   width_shift_range=0.1,\n",
        "                                   height_shift_range=0.1,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,               # target directory\n",
        "    classes=['cats','dogs'], # [0, 1]\n",
        "    target_size=(150,150),   # image resize\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    valid_dir,               # target directory\n",
        "    classes=['cats','dogs'], # [0, 1]\n",
        "    target_size=(150,150),   # image resize\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam # or RMSprop\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=32,\n",
        "                 kernel_size=(3,3),\n",
        "                 activation='relu',\n",
        "                 input_shape=(150,150,3))) # color!\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=64,\n",
        "                 kernel_size=(3,3),\n",
        "                 activation='relu'))\n",
        "\n",
        "model.add(Conv2D(filters=128,\n",
        "                 kernel_size=(3,3),\n",
        "                 activation='relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=128,\n",
        "                 kernel_size=(3,3),\n",
        "                 activation='relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# model.add(Dropout(rate=0.5))\n",
        "\n",
        "model.add(Dense(units=256, activation='relu'))\n",
        "\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator, # 이미지 2000개 / 20개씩 batch = 100번\n",
        "                   steps_per_epoch=100,\n",
        "                   epochs=30,\n",
        "                   validation_data=validation_generator, # 이미지 1000개 / 20개씩 batch = 50번\n",
        "                   validation_steps=50)\n",
        "\n",
        "model.save('./data/cats_dogs_small_cnn_model_augmentation.h5') # val_accuracy: 0.7560\n",
        "\n",
        "# history 객체로 그래프 그려보기\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_acc = history.history['accuracy']\n",
        "valid_acc = history.history['val_accuracy']\n",
        "\n",
        "train_loss = history.history['loss']\n",
        "valid_loss = history.history['val_loss']\n",
        "\n",
        "figure = plt.figure(figsize=(15,5))\n",
        "ax1 = figure.add_subplot(1,2,1)\n",
        "ax2 = figure.add_subplot(1,2,2)\n",
        "\n",
        "ax1.plot(train_acc, color='r', label='train accuracy')\n",
        "ax1.plot(valid_acc, color='b', label='valid accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid()\n",
        "\n",
        "ax2.plot(train_loss, color='r', label='train loss')\n",
        "ax2.plot(valid_loss, color='b', label='valid loss')\n",
        "ax2.legend()\n",
        "ax2.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yPPy179vVbcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전이학습(Transfer Learning)"
      ],
      "metadata": {
        "id": "SRB5eDKaVfqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer Learning\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "model_base = VGG16(weights='imagenet',      # imagenet 경진대회에서 사용한 W를 사용하겠다\n",
        "                  include_top=False,       # imagenet을 학습시킬 때 사용했던 FC layer은 사용하지 않겠다\n",
        "                  input_shape=(150,150,3)) # include_top=False이면 입력 데이터의 사이즈를 넣어줘야 함\n",
        "#                    include_top=True,\n",
        "#                    input_shape=(224,224,3))\n",
        "\n",
        "print(model_base.summary())"
      ],
      "metadata": {
        "id": "Y9fYrGdaVbeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 개와 고양이 데이터를 VGG16에 통과시켜서 Activation Map을 만들자. ndarray 형태로 저장해야 함\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "base_dir = './data/cat_dog_small'\n",
        "train_dir = os.path.join(base_dir, 'train') # ./data/cat_dog_small/train\n",
        "valid_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "def extraction_feature(directory, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, 4, 4, 512)) # 4차원\n",
        "    labels = np.zeros(shape=(sample_count,)) # 1차원\n",
        "    \n",
        "    generator = datagen.flow_from_directory(\n",
        "        directory,\n",
        "        classes=['cats', 'dogs'], # 각 디렉토리의 이미지 레이블 : 고양이 0, 개 1\n",
        "        target_size=(150,150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "    \n",
        "    i = 0\n",
        "    \n",
        "    for x_data_batch, t_data_batch in generator: # batch size(이미지 20개)에 대한 픽셀 데이터\n",
        "        feature_batch = model_base.predict(x_data_batch) # 20개 이미지 특성 추출\n",
        "        features[i*20:(i+1)*20] = feature_batch\n",
        "        labels[i*20:(i+1)*20] = t_data_batch\n",
        "        \n",
        "        i = i + 1\n",
        "        \n",
        "        if i*20 >= sample_count:\n",
        "            break\n",
        "    \n",
        "    return features, labels\n",
        "    \n",
        "train_features, train_labels = extraction_feature(train_dir, 2000)\n",
        "valid_features, valid_labels = extraction_feature(valid_dir, 1000)\n",
        "test_features, test_labels = extraction_feature(test_dir, 1000)"
      ],
      "metadata": {
        "id": "BojdfyNGVbg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 Activation Map을 이용해서 DNN 학습을 하자\n",
        "\n",
        "train_x_data = np.reshape(train_features, (2000, 4*4*512)) # 2차원으로\n",
        "train_t_data = train_labels\n",
        "                          \n",
        "valid_x_data = np.reshape(valid_features, (1000, 4*4*512))\n",
        "valid_t_data = valid_labels\n",
        "\n",
        "test_x_data = np.reshape(test_features, (1000, 4*4*512))\n",
        "test_t_data = test_labels\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(4*4*512,)))\n",
        "model.add(Dense(units=256,\n",
        "               activation='relu'))\n",
        "model.add(Dropout(rate=0.5))\n",
        "model.add(Dense(units=1,\n",
        "               activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_x_data, train_t_data,\n",
        "                   epochs=30,\n",
        "                   batch_size=20,\n",
        "                   validation_data=(valid_x_data, valid_t_data)) # val_accuracy: 0.9050"
      ],
      "metadata": {
        "id": "0MCJfeOIVbjE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
