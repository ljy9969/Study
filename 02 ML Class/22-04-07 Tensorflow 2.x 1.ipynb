{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bada7b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "# display(df.shape) # (42000, 785)\n",
    "# display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045a3b5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 전처리 : 결측치나 이상치 없음. 단, 정규화는 필요함\n",
    "\n",
    "# 이미지 확인\n",
    "figure = plt.figure()\n",
    "ax_arr = [] # python list\n",
    "\n",
    "img_data = df.drop('label', axis=1, inplace=False).values\n",
    "\n",
    "for n in range(10):\n",
    "    ax_arr.append(figure.add_subplot(2,5,n+1))\n",
    "    ax_arr[n].imshow(img_data[n].reshape(28,28), # 28 x 28 픽셀로 변환\n",
    "                     cmap='Greys', # cmap은 흑백 이미지 처리\n",
    "                     interpolation='nearest') # interpolation는 보간법(이미지 깨지지 않도록)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66ca87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False), df['label'], test_size=0.3, random_state=1, stratify=df['label'])\n",
    "\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e290d8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss val : 1.3581478595733643\n",
      "loss val : 0.32358238101005554\n",
      "loss val : 0.291931688785553\n",
      "loss val : 0.2713947594165802\n",
      "loss val : 0.2546168267726898\n",
      "loss val : 0.2409151792526245\n",
      "loss val : 0.2298405021429062\n",
      "loss val : 0.22098416090011597\n",
      "loss val : 0.21395885944366455\n",
      "loss val : 0.2084009349346161\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Tensorflow Implementation\n",
    "sess = tf.Session()\n",
    "\n",
    "onehot_train_t_data = sess.run(tf.one_hot(train_t_data, depth=10)) # class가 0 ~ 9까지 총 10개\n",
    "onehot_test_t_data = sess.run(tf.one_hot(test_t_data, depth=10))\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32) # 독립변수(feature)의 개수\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32) # class의 logistic 개수\n",
    "\n",
    "# Weight, bias\n",
    "W = tf.Variable(tf.random.normal([784,10])) # 독립변수 2개, W 3개. W는 7840개\n",
    "b = tf.Variable(tf.random.normal([10])) # = logistic 개수\n",
    "\n",
    "# Model, Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# Cross Entropy(loss func)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=T))\n",
    "\n",
    "# Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# Session 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 반복 학습\n",
    "num_of_epoch = 1000\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    total_batch = int(norm_train_x_data.shape[0] / batch_size) # 학습 데이터의 개수 / batch\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_x = norm_train_x_data[i*batch_size:(i+1)*batch_size] # [0:100], [100:200] ~. 100개씩 Slicing\n",
    "        batch_y = onehot_train_t_data[i*batch_size:(i+1)*batch_size]\n",
    "        _, loss_val = sess.run([train, loss], feed_dict={X:batch_x, T:batch_y})\n",
    "        \n",
    "    if step % 100 == 0:\n",
    "        print('loss val : {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe0c28de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9070634841918945\n"
     ]
    }
   ],
   "source": [
    "# 성능평가(Accuracy)\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "accuracy_val = sess.run(accuracy, feed_dict={X:norm_test_x_data, T:onehot_test_t_data})\n",
    "print('Accuracy : {}'.format(accuracy_val)) # 0.9070634841918945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb890d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[1.3611453]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "W = tf.random.normal([1], dtype=tf.float32)\n",
    "\n",
    "print(W.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras의 model 만들기의 순서\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# model = tf.keras.models.Sequential()\n",
    "\n",
    "# model.add('input layer') # layer 추가\n",
    "# model.add('output layer')\n",
    "\n",
    "# model.compile() # loss의 종류와 optimizer 종류를 설정\n",
    "\n",
    "# model.fit() # 학습(마치 Sklearn 사용하는 것처럼)\n",
    "\n",
    "# model.evaluate() # 평가와 predict\n",
    "# model.predict()\n",
    "\n",
    "# model.save() # model 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf08f9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a147f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multinomial Classification by Tensorflow Ver. 2.0 - MNIST\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense # Flatten는 input layer, Dense는 output layer용\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "\n",
    "# Data Split\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "571a1acc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow 2.x 구현\n",
    "\n",
    "# model 생성\n",
    "model = Sequential()\n",
    "\n",
    "# layer 추가\n",
    "model.add(Flatten(input_shape=(norm_train_x_data.shape[1],))) # input layer. 튜플로 입력\n",
    "model.add(Dense(units=10, activation='softmax')) # output layer. class와 logistic의 개수는 10\n",
    "\n",
    "# print(model.summary()) # 구해야 할 W의 개수(7850)를 알려줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3db36b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 2.1449 - accuracy: 0.2542 - val_loss: 2.0314 - val_accuracy: 0.3293\n",
      "Epoch 2/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.9380 - accuracy: 0.4080 - val_loss: 1.8467 - val_accuracy: 0.4815\n",
      "Epoch 3/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.7694 - accuracy: 0.5453 - val_loss: 1.6929 - val_accuracy: 0.6031\n",
      "Epoch 4/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.6283 - accuracy: 0.6324 - val_loss: 1.5636 - val_accuracy: 0.6597\n",
      "Epoch 5/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.5094 - accuracy: 0.6773 - val_loss: 1.4546 - val_accuracy: 0.6976\n",
      "Epoch 6/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.4090 - accuracy: 0.7065 - val_loss: 1.3621 - val_accuracy: 0.7223\n",
      "Epoch 7/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.3234 - accuracy: 0.7293 - val_loss: 1.2832 - val_accuracy: 0.7417\n",
      "Epoch 8/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.2503 - accuracy: 0.7477 - val_loss: 1.2155 - val_accuracy: 0.7575\n",
      "Epoch 9/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.1873 - accuracy: 0.7630 - val_loss: 1.1569 - val_accuracy: 0.7687\n",
      "Epoch 10/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.1324 - accuracy: 0.7739 - val_loss: 1.1058 - val_accuracy: 0.7784\n",
      "Epoch 11/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 1.0845 - accuracy: 0.7832 - val_loss: 1.0610 - val_accuracy: 0.7886\n",
      "Epoch 12/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.0422 - accuracy: 0.7910 - val_loss: 1.0213 - val_accuracy: 0.7963\n",
      "Epoch 13/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.0046 - accuracy: 0.7973 - val_loss: 0.9860 - val_accuracy: 0.8044\n",
      "Epoch 14/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9711 - accuracy: 0.8035 - val_loss: 0.9544 - val_accuracy: 0.8092\n",
      "Epoch 15/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9409 - accuracy: 0.8094 - val_loss: 0.9258 - val_accuracy: 0.8134\n",
      "Epoch 16/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9137 - accuracy: 0.8133 - val_loss: 0.9001 - val_accuracy: 0.8179\n",
      "Epoch 17/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8889 - accuracy: 0.8173 - val_loss: 0.8766 - val_accuracy: 0.8223\n",
      "Epoch 18/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8664 - accuracy: 0.8209 - val_loss: 0.8551 - val_accuracy: 0.8264\n",
      "Epoch 19/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8457 - accuracy: 0.8242 - val_loss: 0.8354 - val_accuracy: 0.8298\n",
      "Epoch 20/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8267 - accuracy: 0.8277 - val_loss: 0.8172 - val_accuracy: 0.8318\n",
      "Epoch 21/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8091 - accuracy: 0.8300 - val_loss: 0.8004 - val_accuracy: 0.8350\n",
      "Epoch 22/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7929 - accuracy: 0.8328 - val_loss: 0.7849 - val_accuracy: 0.8362\n",
      "Epoch 23/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7778 - accuracy: 0.8349 - val_loss: 0.7704 - val_accuracy: 0.8364\n",
      "Epoch 24/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7637 - accuracy: 0.8372 - val_loss: 0.7569 - val_accuracy: 0.8383\n",
      "Epoch 25/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.7506 - accuracy: 0.8389 - val_loss: 0.7443 - val_accuracy: 0.8395\n",
      "Epoch 26/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.7383 - accuracy: 0.8403 - val_loss: 0.7325 - val_accuracy: 0.8415\n",
      "Epoch 27/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7267 - accuracy: 0.8417 - val_loss: 0.7214 - val_accuracy: 0.8427\n",
      "Epoch 28/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7158 - accuracy: 0.8437 - val_loss: 0.7110 - val_accuracy: 0.8437\n",
      "Epoch 29/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.7056 - accuracy: 0.8452 - val_loss: 0.7011 - val_accuracy: 0.8452\n",
      "Epoch 30/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6959 - accuracy: 0.8462 - val_loss: 0.6918 - val_accuracy: 0.8468\n",
      "Epoch 31/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6867 - accuracy: 0.8478 - val_loss: 0.6830 - val_accuracy: 0.8480\n",
      "Epoch 32/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6780 - accuracy: 0.8487 - val_loss: 0.6746 - val_accuracy: 0.8490\n",
      "Epoch 33/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6698 - accuracy: 0.8494 - val_loss: 0.6667 - val_accuracy: 0.8517\n",
      "Epoch 34/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6619 - accuracy: 0.8511 - val_loss: 0.6592 - val_accuracy: 0.8529\n",
      "Epoch 35/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6545 - accuracy: 0.8520 - val_loss: 0.6520 - val_accuracy: 0.8553\n",
      "Epoch 36/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6473 - accuracy: 0.8532 - val_loss: 0.6451 - val_accuracy: 0.8561\n",
      "Epoch 37/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6405 - accuracy: 0.8545 - val_loss: 0.6385 - val_accuracy: 0.8570\n",
      "Epoch 38/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6340 - accuracy: 0.8550 - val_loss: 0.6323 - val_accuracy: 0.8575\n",
      "Epoch 39/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6278 - accuracy: 0.8560 - val_loss: 0.6263 - val_accuracy: 0.8585\n",
      "Epoch 40/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6218 - accuracy: 0.8569 - val_loss: 0.6205 - val_accuracy: 0.8609\n",
      "Epoch 41/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6161 - accuracy: 0.8580 - val_loss: 0.6150 - val_accuracy: 0.8614\n",
      "Epoch 42/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6106 - accuracy: 0.8588 - val_loss: 0.6096 - val_accuracy: 0.8624\n",
      "Epoch 43/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6053 - accuracy: 0.8594 - val_loss: 0.6046 - val_accuracy: 0.8634\n",
      "Epoch 44/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6002 - accuracy: 0.8597 - val_loss: 0.5997 - val_accuracy: 0.8636\n",
      "Epoch 45/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5954 - accuracy: 0.8605 - val_loss: 0.5950 - val_accuracy: 0.8648\n",
      "Epoch 46/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5906 - accuracy: 0.8612 - val_loss: 0.5904 - val_accuracy: 0.8648\n",
      "Epoch 47/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5861 - accuracy: 0.8624 - val_loss: 0.5860 - val_accuracy: 0.8650\n",
      "Epoch 48/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5817 - accuracy: 0.8628 - val_loss: 0.5818 - val_accuracy: 0.8653\n",
      "Epoch 49/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5775 - accuracy: 0.8637 - val_loss: 0.5777 - val_accuracy: 0.8656\n",
      "Epoch 50/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5734 - accuracy: 0.8639 - val_loss: 0.5737 - val_accuracy: 0.8662\n",
      "Epoch 51/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5694 - accuracy: 0.8647 - val_loss: 0.5699 - val_accuracy: 0.8665\n",
      "Epoch 52/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5656 - accuracy: 0.8653 - val_loss: 0.5663 - val_accuracy: 0.8667\n",
      "Epoch 53/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5618 - accuracy: 0.8659 - val_loss: 0.5626 - val_accuracy: 0.8675\n",
      "Epoch 54/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5582 - accuracy: 0.8661 - val_loss: 0.5592 - val_accuracy: 0.8679\n",
      "Epoch 55/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5548 - accuracy: 0.8665 - val_loss: 0.5558 - val_accuracy: 0.8682\n",
      "Epoch 56/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5514 - accuracy: 0.8673 - val_loss: 0.5526 - val_accuracy: 0.8696\n",
      "Epoch 57/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5481 - accuracy: 0.8679 - val_loss: 0.5494 - val_accuracy: 0.8697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5449 - accuracy: 0.8685 - val_loss: 0.5463 - val_accuracy: 0.8696\n",
      "Epoch 59/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5418 - accuracy: 0.8691 - val_loss: 0.5433 - val_accuracy: 0.8694\n",
      "Epoch 60/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5388 - accuracy: 0.8695 - val_loss: 0.5404 - val_accuracy: 0.8697\n",
      "Epoch 61/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5358 - accuracy: 0.8699 - val_loss: 0.5376 - val_accuracy: 0.8701\n",
      "Epoch 62/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5330 - accuracy: 0.8702 - val_loss: 0.5348 - val_accuracy: 0.8701\n",
      "Epoch 63/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5302 - accuracy: 0.8709 - val_loss: 0.5321 - val_accuracy: 0.8702\n",
      "Epoch 64/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5275 - accuracy: 0.8711 - val_loss: 0.5295 - val_accuracy: 0.8704\n",
      "Epoch 65/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5248 - accuracy: 0.8714 - val_loss: 0.5270 - val_accuracy: 0.8707\n",
      "Epoch 66/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5222 - accuracy: 0.8722 - val_loss: 0.5245 - val_accuracy: 0.8713\n",
      "Epoch 67/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5197 - accuracy: 0.8730 - val_loss: 0.5221 - val_accuracy: 0.8714\n",
      "Epoch 68/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5173 - accuracy: 0.8731 - val_loss: 0.5197 - val_accuracy: 0.8718\n",
      "Epoch 69/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5149 - accuracy: 0.8734 - val_loss: 0.5174 - val_accuracy: 0.8716\n",
      "Epoch 70/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5126 - accuracy: 0.8740 - val_loss: 0.5152 - val_accuracy: 0.8721\n",
      "Epoch 71/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5103 - accuracy: 0.8746 - val_loss: 0.5130 - val_accuracy: 0.8724\n",
      "Epoch 72/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5081 - accuracy: 0.8752 - val_loss: 0.5108 - val_accuracy: 0.8726\n",
      "Epoch 73/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5059 - accuracy: 0.8753 - val_loss: 0.5087 - val_accuracy: 0.8728\n",
      "Epoch 74/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5038 - accuracy: 0.8759 - val_loss: 0.5067 - val_accuracy: 0.8731\n",
      "Epoch 75/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5017 - accuracy: 0.8760 - val_loss: 0.5047 - val_accuracy: 0.8736\n",
      "Epoch 76/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4996 - accuracy: 0.8763 - val_loss: 0.5027 - val_accuracy: 0.8738\n",
      "Epoch 77/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4976 - accuracy: 0.8766 - val_loss: 0.5008 - val_accuracy: 0.8741\n",
      "Epoch 78/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4957 - accuracy: 0.8767 - val_loss: 0.4989 - val_accuracy: 0.8747\n",
      "Epoch 79/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4938 - accuracy: 0.8771 - val_loss: 0.4971 - val_accuracy: 0.8750\n",
      "Epoch 80/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4919 - accuracy: 0.8776 - val_loss: 0.4953 - val_accuracy: 0.8757\n",
      "Epoch 81/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4901 - accuracy: 0.8776 - val_loss: 0.4936 - val_accuracy: 0.8757\n",
      "Epoch 82/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4883 - accuracy: 0.8782 - val_loss: 0.4918 - val_accuracy: 0.8760\n",
      "Epoch 83/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4865 - accuracy: 0.8785 - val_loss: 0.4901 - val_accuracy: 0.8760\n",
      "Epoch 84/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4848 - accuracy: 0.8789 - val_loss: 0.4885 - val_accuracy: 0.8762\n",
      "Epoch 85/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4831 - accuracy: 0.8790 - val_loss: 0.4869 - val_accuracy: 0.8765\n",
      "Epoch 86/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4814 - accuracy: 0.8793 - val_loss: 0.4853 - val_accuracy: 0.8769\n",
      "Epoch 87/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4798 - accuracy: 0.8793 - val_loss: 0.4837 - val_accuracy: 0.8772\n",
      "Epoch 88/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4782 - accuracy: 0.8800 - val_loss: 0.4822 - val_accuracy: 0.8770\n",
      "Epoch 89/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4766 - accuracy: 0.8801 - val_loss: 0.4807 - val_accuracy: 0.8774\n",
      "Epoch 90/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4751 - accuracy: 0.8805 - val_loss: 0.4792 - val_accuracy: 0.8777\n",
      "Epoch 91/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4736 - accuracy: 0.8806 - val_loss: 0.4778 - val_accuracy: 0.8779\n",
      "Epoch 92/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4721 - accuracy: 0.8810 - val_loss: 0.4763 - val_accuracy: 0.8779\n",
      "Epoch 93/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4706 - accuracy: 0.8813 - val_loss: 0.4749 - val_accuracy: 0.8786\n",
      "Epoch 94/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4692 - accuracy: 0.8814 - val_loss: 0.4735 - val_accuracy: 0.8786\n",
      "Epoch 95/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4678 - accuracy: 0.8817 - val_loss: 0.4722 - val_accuracy: 0.8791\n",
      "Epoch 96/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4664 - accuracy: 0.8821 - val_loss: 0.4708 - val_accuracy: 0.8794\n",
      "Epoch 97/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4650 - accuracy: 0.8824 - val_loss: 0.4695 - val_accuracy: 0.8794\n",
      "Epoch 98/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4637 - accuracy: 0.8827 - val_loss: 0.4682 - val_accuracy: 0.8796\n",
      "Epoch 99/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4624 - accuracy: 0.8828 - val_loss: 0.4670 - val_accuracy: 0.8798\n",
      "Epoch 100/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4611 - accuracy: 0.8830 - val_loss: 0.4657 - val_accuracy: 0.8798\n"
     ]
    }
   ],
   "source": [
    "# model compile : 사용할 loss 함수를 지정, 사용할 optimizer(알고리즘)를 지정\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# loss\n",
    "# linear regression : linear(MSE)\n",
    "# binary classification : binary_crossentropy\n",
    "# multinomial classification : categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy', # sparse로 원핫인코딩 처리까지\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 학습 결과를 변수에 저장\n",
    "history = model.fit(norm_train_x_data, train_t_data,\n",
    "                    epochs=100, batch_size=100,\n",
    "                    verbose=1, validation_split=0.2) # val_accuracy가 높아야! verbose는 출력 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d3ef101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 0s 676us/step - loss: 0.4788 - accuracy: 0.8767\n",
      "[0.4787802994251251, 0.8767460584640503]\n"
     ]
    }
   ],
   "source": [
    "# 최종 평가\n",
    "print(model.evaluate(norm_test_x_data, test_t_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e86a0b8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "185/236 [======================>.......] - ETA: 0s - loss: 2.3350 - accuracy: 0.1027\n",
      "Epoch 00001: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 2.3049 - accuracy: 0.1175 - val_loss: 2.1652 - val_accuracy: 0.2044\n",
      "Epoch 2/100\n",
      "228/236 [===========================>..] - ETA: 0s - loss: 2.0540 - accuracy: 0.3075\n",
      "Epoch 00002: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 2.0508 - accuracy: 0.3110 - val_loss: 1.9493 - val_accuracy: 0.4158\n",
      "Epoch 3/100\n",
      "193/236 [=======================>......] - ETA: 0s - loss: 1.8753 - accuracy: 0.4860\n",
      "Epoch 00003: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.8588 - accuracy: 0.4997 - val_loss: 1.7767 - val_accuracy: 0.5626\n",
      "Epoch 4/100\n",
      "224/236 [===========================>..] - ETA: 0s - loss: 1.7057 - accuracy: 0.6117\n",
      "Epoch 00004: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 1.7016 - accuracy: 0.6139 - val_loss: 1.6332 - val_accuracy: 0.6464\n",
      "Epoch 5/100\n",
      "194/236 [=======================>......] - ETA: 0s - loss: 1.5820 - accuracy: 0.6673\n",
      "Epoch 00005: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 1.5700 - accuracy: 0.6731 - val_loss: 1.5125 - val_accuracy: 0.6878\n",
      "Epoch 6/100\n",
      "192/236 [=======================>......] - ETA: 0s - loss: 1.4674 - accuracy: 0.7062\n",
      "Epoch 00006: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 1.4591 - accuracy: 0.7077 - val_loss: 1.4108 - val_accuracy: 0.7146\n",
      "Epoch 7/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 1.3655 - accuracy: 0.7304\n",
      "Epoch 00007: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 1.3654 - accuracy: 0.7305 - val_loss: 1.3244 - val_accuracy: 0.7361\n",
      "Epoch 8/100\n",
      "206/236 [=========================>....] - ETA: 0s - loss: 1.2910 - accuracy: 0.7458\n",
      "Epoch 00008: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.2856 - accuracy: 0.7479 - val_loss: 1.2506 - val_accuracy: 0.7556\n",
      "Epoch 9/100\n",
      "222/236 [===========================>..] - ETA: 0s - loss: 1.2197 - accuracy: 0.7608\n",
      "Epoch 00009: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.2172 - accuracy: 0.7613 - val_loss: 1.1870 - val_accuracy: 0.7660\n",
      "Epoch 10/100\n",
      "217/236 [==========================>...] - ETA: 0s - loss: 1.1612 - accuracy: 0.7701\n",
      "Epoch 00010: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.1580 - accuracy: 0.7712 - val_loss: 1.1318 - val_accuracy: 0.7764\n",
      "Epoch 11/100\n",
      "210/236 [=========================>....] - ETA: 0s - loss: 1.1102 - accuracy: 0.7808\n",
      "Epoch 00011: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.1065 - accuracy: 0.7808 - val_loss: 1.0836 - val_accuracy: 0.7823\n",
      "Epoch 12/100\n",
      "190/236 [=======================>......] - ETA: 0s - loss: 1.0655 - accuracy: 0.7884\n",
      "Epoch 00012: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.0614 - accuracy: 0.7894 - val_loss: 1.0412 - val_accuracy: 0.7917\n",
      "Epoch 13/100\n",
      "223/236 [===========================>..] - ETA: 0s - loss: 1.0232 - accuracy: 0.7953\n",
      "Epoch 00013: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.0215 - accuracy: 0.7959 - val_loss: 1.0036 - val_accuracy: 0.7964\n",
      "Epoch 14/100\n",
      "207/236 [=========================>....] - ETA: 0s - loss: 0.9895 - accuracy: 0.8011\n",
      "Epoch 00014: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9860 - accuracy: 0.8021 - val_loss: 0.9701 - val_accuracy: 0.8031\n",
      "Epoch 15/100\n",
      "189/236 [=======================>......] - ETA: 0s - loss: 0.9578 - accuracy: 0.8049\n",
      "Epoch 00015: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9542 - accuracy: 0.8062 - val_loss: 0.9399 - val_accuracy: 0.8063\n",
      "Epoch 16/100\n",
      "232/236 [============================>.] - ETA: 0s - loss: 0.9246 - accuracy: 0.8117\n",
      "Epoch 00016: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9256 - accuracy: 0.8108 - val_loss: 0.9128 - val_accuracy: 0.8114\n",
      "Epoch 17/100\n",
      "227/236 [===========================>..] - ETA: 0s - loss: 0.9000 - accuracy: 0.8150\n",
      "Epoch 00017: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8997 - accuracy: 0.8151 - val_loss: 0.8881 - val_accuracy: 0.8151\n",
      "Epoch 18/100\n",
      "215/236 [==========================>...] - ETA: 0s - loss: 0.8753 - accuracy: 0.8194\n",
      "Epoch 00018: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.8761 - accuracy: 0.8184 - val_loss: 0.8656 - val_accuracy: 0.8185\n",
      "Epoch 19/100\n",
      "215/236 [==========================>...] - ETA: 0s - loss: 0.8536 - accuracy: 0.8213\n",
      "Epoch 00019: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8546 - accuracy: 0.8216 - val_loss: 0.8450 - val_accuracy: 0.8224\n",
      "Epoch 20/100\n",
      "200/236 [========================>.....] - ETA: 0s - loss: 0.8340 - accuracy: 0.8250\n",
      "Epoch 00020: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8348 - accuracy: 0.8246 - val_loss: 0.8261 - val_accuracy: 0.8253\n",
      "Epoch 21/100\n",
      "195/236 [=======================>......] - ETA: 0s - loss: 0.8184 - accuracy: 0.8258\n",
      "Epoch 00021: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8166 - accuracy: 0.8278 - val_loss: 0.8087 - val_accuracy: 0.8277\n",
      "Epoch 22/100\n",
      "189/236 [=======================>......] - ETA: 0s - loss: 0.8014 - accuracy: 0.8299\n",
      "Epoch 00022: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7998 - accuracy: 0.8299 - val_loss: 0.7926 - val_accuracy: 0.8306\n",
      "Epoch 23/100\n",
      "209/236 [=========================>....] - ETA: 0s - loss: 0.7834 - accuracy: 0.8335\n",
      "Epoch 00023: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7842 - accuracy: 0.8327 - val_loss: 0.7776 - val_accuracy: 0.8332\n",
      "Epoch 24/100\n",
      "198/236 [========================>.....] - ETA: 0s - loss: 0.7702 - accuracy: 0.8346\n",
      "Epoch 00024: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7697 - accuracy: 0.8341 - val_loss: 0.7636 - val_accuracy: 0.8361\n",
      "Epoch 25/100\n",
      "203/236 [========================>.....] - ETA: 0s - loss: 0.7555 - accuracy: 0.8356\n",
      "Epoch 00025: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7562 - accuracy: 0.8359 - val_loss: 0.7506 - val_accuracy: 0.8378\n",
      "Epoch 26/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.7471 - accuracy: 0.8363\n",
      "Epoch 00026: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7436 - accuracy: 0.8378 - val_loss: 0.7384 - val_accuracy: 0.8391\n",
      "Epoch 27/100\n",
      "198/236 [========================>.....] - ETA: 0s - loss: 0.7304 - accuracy: 0.8397\n",
      "Epoch 00027: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7317 - accuracy: 0.8396 - val_loss: 0.7270 - val_accuracy: 0.8410\n",
      "Epoch 28/100\n",
      "210/236 [=========================>....] - ETA: 0s - loss: 0.7218 - accuracy: 0.8415\n",
      "Epoch 00028: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7206 - accuracy: 0.8416 - val_loss: 0.7163 - val_accuracy: 0.8413\n",
      "Epoch 29/100\n",
      "206/236 [=========================>....] - ETA: 0s - loss: 0.7090 - accuracy: 0.8449\n",
      "Epoch 00029: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7101 - accuracy: 0.8434 - val_loss: 0.7062 - val_accuracy: 0.8425\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213/236 [==========================>...] - ETA: 0s - loss: 0.7004 - accuracy: 0.8447\n",
      "Epoch 00030: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7001 - accuracy: 0.8445 - val_loss: 0.6966 - val_accuracy: 0.8440\n",
      "Epoch 31/100\n",
      "208/236 [=========================>....] - ETA: 0s - loss: 0.6896 - accuracy: 0.8450\n",
      "Epoch 00031: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6908 - accuracy: 0.8459 - val_loss: 0.6876 - val_accuracy: 0.8449\n",
      "Epoch 32/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.6823 - accuracy: 0.8467\n",
      "Epoch 00032: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6819 - accuracy: 0.8470 - val_loss: 0.6790 - val_accuracy: 0.8454\n",
      "Epoch 33/100\n",
      "202/236 [========================>.....] - ETA: 0s - loss: 0.6761 - accuracy: 0.8478\n",
      "Epoch 00033: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6735 - accuracy: 0.8486 - val_loss: 0.6709 - val_accuracy: 0.8469\n",
      "Epoch 34/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.6657 - accuracy: 0.8494\n",
      "Epoch 00034: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6655 - accuracy: 0.8494 - val_loss: 0.6632 - val_accuracy: 0.8480\n",
      "Epoch 35/100\n",
      "205/236 [=========================>....] - ETA: 0s - loss: 0.6594 - accuracy: 0.8510\n",
      "Epoch 00035: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6579 - accuracy: 0.8511 - val_loss: 0.6558 - val_accuracy: 0.8497\n",
      "Epoch 36/100\n",
      "204/236 [========================>.....] - ETA: 0s - loss: 0.6510 - accuracy: 0.8530\n",
      "Epoch 00036: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6506 - accuracy: 0.8524 - val_loss: 0.6488 - val_accuracy: 0.8510\n",
      "Epoch 37/100\n",
      "221/236 [===========================>..] - ETA: 0s - loss: 0.6427 - accuracy: 0.8536\n",
      "Epoch 00037: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6437 - accuracy: 0.8533 - val_loss: 0.6421 - val_accuracy: 0.8515\n",
      "Epoch 38/100\n",
      "208/236 [=========================>....] - ETA: 0s - loss: 0.6396 - accuracy: 0.8534\n",
      "Epoch 00038: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6371 - accuracy: 0.8545 - val_loss: 0.6356 - val_accuracy: 0.8532\n",
      "Epoch 39/100\n",
      "212/236 [=========================>....] - ETA: 0s - loss: 0.6312 - accuracy: 0.8549\n",
      "Epoch 00039: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6307 - accuracy: 0.8557 - val_loss: 0.6296 - val_accuracy: 0.8534\n",
      "Epoch 40/100\n",
      "216/236 [==========================>...] - ETA: 0s - loss: 0.6260 - accuracy: 0.8560\n",
      "Epoch 00040: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6247 - accuracy: 0.8564 - val_loss: 0.6237 - val_accuracy: 0.8548\n",
      "Epoch 41/100\n",
      "195/236 [=======================>......] - ETA: 0s - loss: 0.6205 - accuracy: 0.8559\n",
      "Epoch 00041: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6189 - accuracy: 0.8573 - val_loss: 0.6181 - val_accuracy: 0.8554\n",
      "Epoch 42/100\n",
      "205/236 [=========================>....] - ETA: 0s - loss: 0.6149 - accuracy: 0.8575\n",
      "Epoch 00042: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6133 - accuracy: 0.8580 - val_loss: 0.6127 - val_accuracy: 0.8560\n",
      "Epoch 43/100\n",
      "182/236 [======================>.......] - ETA: 0s - loss: 0.6079 - accuracy: 0.8580\n",
      "Epoch 00043: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6079 - accuracy: 0.8584 - val_loss: 0.6075 - val_accuracy: 0.8571\n",
      "Epoch 44/100\n",
      "190/236 [=======================>......] - ETA: 0s - loss: 0.6033 - accuracy: 0.8597\n",
      "Epoch 00044: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6028 - accuracy: 0.8595 - val_loss: 0.6025 - val_accuracy: 0.8578\n",
      "Epoch 45/100\n",
      "191/236 [=======================>......] - ETA: 0s - loss: 0.6040 - accuracy: 0.8572\n",
      "Epoch 00045: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5978 - accuracy: 0.8602 - val_loss: 0.5977 - val_accuracy: 0.8594\n",
      "Epoch 46/100\n",
      "196/236 [=======================>......] - ETA: 0s - loss: 0.5901 - accuracy: 0.8613\n",
      "Epoch 00046: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5930 - accuracy: 0.8608 - val_loss: 0.5931 - val_accuracy: 0.8590\n",
      "Epoch 47/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.5892 - accuracy: 0.8621\n",
      "Epoch 00047: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5884 - accuracy: 0.8617 - val_loss: 0.5887 - val_accuracy: 0.8602\n",
      "Epoch 48/100\n",
      "215/236 [==========================>...] - ETA: 0s - loss: 0.5849 - accuracy: 0.8627\n",
      "Epoch 00048: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5840 - accuracy: 0.8625 - val_loss: 0.5843 - val_accuracy: 0.8611\n",
      "Epoch 49/100\n",
      "213/236 [==========================>...] - ETA: 0s - loss: 0.5786 - accuracy: 0.8625\n",
      "Epoch 00049: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5797 - accuracy: 0.8629 - val_loss: 0.5802 - val_accuracy: 0.8621\n",
      "Epoch 50/100\n",
      "208/236 [=========================>....] - ETA: 0s - loss: 0.5766 - accuracy: 0.8631\n",
      "Epoch 00050: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5755 - accuracy: 0.8635 - val_loss: 0.5762 - val_accuracy: 0.8624\n",
      "Epoch 51/100\n",
      "228/236 [===========================>..] - ETA: 0s - loss: 0.5728 - accuracy: 0.8636\n",
      "Epoch 00051: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5715 - accuracy: 0.8637 - val_loss: 0.5723 - val_accuracy: 0.8629\n",
      "Epoch 52/100\n",
      "215/236 [==========================>...] - ETA: 0s - loss: 0.5682 - accuracy: 0.8645\n",
      "Epoch 00052: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5676 - accuracy: 0.8648 - val_loss: 0.5686 - val_accuracy: 0.8639\n",
      "Epoch 53/100\n",
      "209/236 [=========================>....] - ETA: 0s - loss: 0.5631 - accuracy: 0.8658\n",
      "Epoch 00053: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5639 - accuracy: 0.8650 - val_loss: 0.5649 - val_accuracy: 0.8641\n",
      "Epoch 54/100\n",
      "230/236 [============================>.] - ETA: 0s - loss: 0.5614 - accuracy: 0.8648\n",
      "Epoch 00054: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5602 - accuracy: 0.8655 - val_loss: 0.5614 - val_accuracy: 0.8643\n",
      "Epoch 55/100\n",
      "213/236 [==========================>...] - ETA: 0s - loss: 0.5572 - accuracy: 0.8667\n",
      "Epoch 00055: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5567 - accuracy: 0.8665 - val_loss: 0.5580 - val_accuracy: 0.8646\n",
      "Epoch 56/100\n",
      "203/236 [========================>.....] - ETA: 0s - loss: 0.5542 - accuracy: 0.8674\n",
      "Epoch 00056: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5533 - accuracy: 0.8671 - val_loss: 0.5547 - val_accuracy: 0.8648\n",
      "Epoch 57/100\n",
      "199/236 [========================>.....] - ETA: 0s - loss: 0.5470 - accuracy: 0.8679\n",
      "Epoch 00057: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5500 - accuracy: 0.8677 - val_loss: 0.5515 - val_accuracy: 0.8650\n",
      "Epoch 58/100\n",
      "198/236 [========================>.....] - ETA: 0s - loss: 0.5475 - accuracy: 0.8674\n",
      "Epoch 00058: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5467 - accuracy: 0.8681 - val_loss: 0.5484 - val_accuracy: 0.8650\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/236 [==========================>...] - ETA: 0s - loss: 0.5434 - accuracy: 0.8688\n",
      "Epoch 00059: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5436 - accuracy: 0.8688 - val_loss: 0.5454 - val_accuracy: 0.8645\n",
      "Epoch 60/100\n",
      "229/236 [============================>.] - ETA: 0s - loss: 0.5413 - accuracy: 0.8690\n",
      "Epoch 00060: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5406 - accuracy: 0.8691 - val_loss: 0.5424 - val_accuracy: 0.8650\n",
      "Epoch 61/100\n",
      "199/236 [========================>.....] - ETA: 0s - loss: 0.5397 - accuracy: 0.8693\n",
      "Epoch 00061: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5376 - accuracy: 0.8699 - val_loss: 0.5396 - val_accuracy: 0.8653\n",
      "Epoch 62/100\n",
      "221/236 [===========================>..] - ETA: 0s - loss: 0.5368 - accuracy: 0.8696\n",
      "Epoch 00062: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.8703 - val_loss: 0.5368 - val_accuracy: 0.8663\n",
      "Epoch 63/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.5319 - accuracy: 0.8707\n",
      "Epoch 00063: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5319 - accuracy: 0.8707 - val_loss: 0.5341 - val_accuracy: 0.8665\n",
      "Epoch 64/100\n",
      "191/236 [=======================>......] - ETA: 0s - loss: 0.5308 - accuracy: 0.8699\n",
      "Epoch 00064: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5292 - accuracy: 0.8709 - val_loss: 0.5314 - val_accuracy: 0.8670\n",
      "Epoch 65/100\n",
      "193/236 [=======================>......] - ETA: 0s - loss: 0.5262 - accuracy: 0.8716\n",
      "Epoch 00065: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5265 - accuracy: 0.8719 - val_loss: 0.5289 - val_accuracy: 0.8672\n",
      "Epoch 66/100\n",
      "183/236 [======================>.......] - ETA: 0s - loss: 0.5256 - accuracy: 0.8720\n",
      "Epoch 00066: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.8721 - val_loss: 0.5264 - val_accuracy: 0.8680\n",
      "Epoch 67/100\n",
      "189/236 [=======================>......] - ETA: 0s - loss: 0.5216 - accuracy: 0.8728\n",
      "Epoch 00067: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5214 - accuracy: 0.8727 - val_loss: 0.5239 - val_accuracy: 0.8680\n",
      "Epoch 68/100\n",
      "215/236 [==========================>...] - ETA: 0s - loss: 0.5203 - accuracy: 0.8733\n",
      "Epoch 00068: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5189 - accuracy: 0.8732 - val_loss: 0.5215 - val_accuracy: 0.8696\n",
      "Epoch 69/100\n",
      "205/236 [=========================>....] - ETA: 0s - loss: 0.5150 - accuracy: 0.8745\n",
      "Epoch 00069: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.8736 - val_loss: 0.5192 - val_accuracy: 0.8699\n",
      "Epoch 70/100\n",
      "229/236 [============================>.] - ETA: 0s - loss: 0.5148 - accuracy: 0.8739\n",
      "Epoch 00070: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5141 - accuracy: 0.8740 - val_loss: 0.5169 - val_accuracy: 0.8702\n",
      "Epoch 71/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.5121 - accuracy: 0.8738\n",
      "Epoch 00071: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.8742 - val_loss: 0.5147 - val_accuracy: 0.8706\n",
      "Epoch 72/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.5094 - accuracy: 0.8747\n",
      "Epoch 00072: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.8746 - val_loss: 0.5126 - val_accuracy: 0.8709\n",
      "Epoch 73/100\n",
      "196/236 [=======================>......] - ETA: 0s - loss: 0.5064 - accuracy: 0.8759\n",
      "Epoch 00073: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5074 - accuracy: 0.8749 - val_loss: 0.5105 - val_accuracy: 0.8711\n",
      "Epoch 74/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.8752\n",
      "Epoch 00074: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5052 - accuracy: 0.8752 - val_loss: 0.5084 - val_accuracy: 0.8719\n",
      "Epoch 75/100\n",
      "230/236 [============================>.] - ETA: 0s - loss: 0.5028 - accuracy: 0.8759\n",
      "Epoch 00075: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.8756 - val_loss: 0.5064 - val_accuracy: 0.8723\n",
      "Epoch 76/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.5006 - accuracy: 0.8764\n",
      "Epoch 00076: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.8759 - val_loss: 0.5044 - val_accuracy: 0.8726\n",
      "Epoch 77/100\n",
      "216/236 [==========================>...] - ETA: 0s - loss: 0.4972 - accuracy: 0.8767\n",
      "Epoch 00077: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4991 - accuracy: 0.8763 - val_loss: 0.5025 - val_accuracy: 0.8728\n",
      "Epoch 78/100\n",
      "222/236 [===========================>..] - ETA: 0s - loss: 0.4946 - accuracy: 0.8774 ETA: 0s - loss: 0.5007 - accuracy: 0.\n",
      "Epoch 00078: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4971 - accuracy: 0.8765 - val_loss: 0.5006 - val_accuracy: 0.8733\n",
      "Epoch 79/100\n",
      "210/236 [=========================>....] - ETA: 0s - loss: 0.4946 - accuracy: 0.8772\n",
      "Epoch 00079: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4952 - accuracy: 0.8769 - val_loss: 0.4988 - val_accuracy: 0.8731\n",
      "Epoch 80/100\n",
      "231/236 [============================>.] - ETA: 0s - loss: 0.4935 - accuracy: 0.8774\n",
      "Epoch 00080: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4933 - accuracy: 0.8772 - val_loss: 0.4969 - val_accuracy: 0.8736\n",
      "Epoch 81/100\n",
      "227/236 [===========================>..] - ETA: 0s - loss: 0.4903 - accuracy: 0.8778 ETA: 0s - loss: 0.4942 - accuracy: \n",
      "Epoch 00081: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.8771 - val_loss: 0.4952 - val_accuracy: 0.8735\n",
      "Epoch 82/100\n",
      "215/236 [==========================>...] - ETA: 0s - loss: 0.4909 - accuracy: 0.8767\n",
      "Epoch 00082: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4897 - accuracy: 0.8775 - val_loss: 0.4934 - val_accuracy: 0.8736\n",
      "Epoch 83/100\n",
      "208/236 [=========================>....] - ETA: 0s - loss: 0.4916 - accuracy: 0.8772 ETA: 0s - loss: 0.4973 - accuracy: 0.\n",
      "Epoch 00083: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.8777 - val_loss: 0.4917 - val_accuracy: 0.8743\n",
      "Epoch 84/100\n",
      "225/236 [===========================>..] - ETA: 0s - loss: 0.4851 - accuracy: 0.8789\n",
      "Epoch 00084: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4861 - accuracy: 0.8780 - val_loss: 0.4900 - val_accuracy: 0.8745\n",
      "Epoch 85/100\n",
      "214/236 [==========================>...] - ETA: 0s - loss: 0.4855 - accuracy: 0.8786\n",
      "Epoch 00085: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.8782 - val_loss: 0.4884 - val_accuracy: 0.8750\n",
      "Epoch 86/100\n",
      "185/236 [======================>.......] - ETA: 0s - loss: 0.4839 - accuracy: 0.8770\n",
      "Epoch 00086: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.8785 - val_loss: 0.4868 - val_accuracy: 0.8753\n",
      "Epoch 87/100\n",
      "227/236 [===========================>..] - ETA: 0s - loss: 0.4817 - accuracy: 0.8784\n",
      "Epoch 00087: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4811 - accuracy: 0.8787 - val_loss: 0.4852 - val_accuracy: 0.8755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.4758 - accuracy: 0.8816\n",
      "Epoch 00088: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.8788 - val_loss: 0.4836 - val_accuracy: 0.8759\n",
      "Epoch 89/100\n",
      "231/236 [============================>.] - ETA: 0s - loss: 0.4775 - accuracy: 0.8795\n",
      "Epoch 00089: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4779 - accuracy: 0.8792 - val_loss: 0.4821 - val_accuracy: 0.8762\n",
      "Epoch 90/100\n",
      "212/236 [=========================>....] - ETA: 0s - loss: 0.4751 - accuracy: 0.8796\n",
      "Epoch 00090: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4764 - accuracy: 0.8795 - val_loss: 0.4807 - val_accuracy: 0.8765\n",
      "Epoch 91/100\n",
      "191/236 [=======================>......] - ETA: 0s - loss: 0.4745 - accuracy: 0.8788\n",
      "Epoch 00091: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4749 - accuracy: 0.8799 - val_loss: 0.4792 - val_accuracy: 0.8762\n",
      "Epoch 92/100\n",
      "184/236 [======================>.......] - ETA: 0s - loss: 0.4744 - accuracy: 0.8788\n",
      "Epoch 00092: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4734 - accuracy: 0.8802 - val_loss: 0.4778 - val_accuracy: 0.8764\n",
      "Epoch 93/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.4716 - accuracy: 0.8807\n",
      "Epoch 00093: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.8804 - val_loss: 0.4764 - val_accuracy: 0.8767\n",
      "Epoch 94/100\n",
      "210/236 [=========================>....] - ETA: 0s - loss: 0.4706 - accuracy: 0.8799\n",
      "Epoch 00094: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4704 - accuracy: 0.8807 - val_loss: 0.4750 - val_accuracy: 0.8769\n",
      "Epoch 95/100\n",
      "201/236 [========================>.....] - ETA: 0s - loss: 0.4704 - accuracy: 0.8803\n",
      "Epoch 00095: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4690 - accuracy: 0.8809 - val_loss: 0.4736 - val_accuracy: 0.8774\n",
      "Epoch 96/100\n",
      "216/236 [==========================>...] - ETA: 0s - loss: 0.4683 - accuracy: 0.8810\n",
      "Epoch 00096: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4676 - accuracy: 0.8810 - val_loss: 0.4723 - val_accuracy: 0.8776\n",
      "Epoch 97/100\n",
      "194/236 [=======================>......] - ETA: 0s - loss: 0.4661 - accuracy: 0.8827\n",
      "Epoch 00097: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4663 - accuracy: 0.8816 - val_loss: 0.4710 - val_accuracy: 0.8776\n",
      "Epoch 98/100\n",
      "221/236 [===========================>..] - ETA: 0s - loss: 0.4658 - accuracy: 0.8814\n",
      "Epoch 00098: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4649 - accuracy: 0.8818 - val_loss: 0.4697 - val_accuracy: 0.8779\n",
      "Epoch 99/100\n",
      "214/236 [==========================>...] - ETA: 0s - loss: 0.4652 - accuracy: 0.8815\n",
      "Epoch 00099: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4636 - accuracy: 0.8820 - val_loss: 0.4684 - val_accuracy: 0.8782\n",
      "Epoch 100/100\n",
      "219/236 [==========================>...] - ETA: 0s - loss: 0.4627 - accuracy: 0.8819\n",
      "Epoch 00100: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4623 - accuracy: 0.8824 - val_loss: 0.4672 - val_accuracy: 0.8782\n",
      "394/394 [==============================] - 0s 732us/step - loss: 0.4814 - accuracy: 0.8759\n",
      "[0.4814387261867523, 0.8758730292320251]\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense # Flatten는 input layer, Dense는 output layer용\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "\n",
    "# 기존에는 test_x_data, test_t_data 이 두 데이터를 validation 용도로 사용함. 이제는 test 용도로 사용할 것\n",
    "# Keras는 학습할 때 train data를 일정 부분 나누어 자체 평가가 가능\n",
    "\n",
    "# Data Split\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)\n",
    "\n",
    "# loss 함수를 sparse_categorical_crossentropy로 지정할 것이기 때문에 label에 대한 one-hot endoding 처리가 필요 없음\n",
    "\n",
    "# model 생성\n",
    "model = Sequential()\n",
    "\n",
    "# layer 추가\n",
    "model.add(Flatten(input_shape=(norm_train_x_data.shape[1],))) # input layer. 튜플로 입력\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model 저장. 구조 빼고 checkpoint 이용해 W, b만 저장. 어디에 저장할지 알려줌\n",
    "checkpoint_path = './training_ckpt/cp.ckpt'\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n",
    "\n",
    "# 학습 결과를 변수에 저장\n",
    "history = model.fit(norm_train_x_data, train_t_data,\n",
    "                    epochs=100, batch_size=100,\n",
    "                    verbose=1, validation_split=0.2, # verbose=0이면 출력 안됨\n",
    "                    callbacks=[cp_callback]) # 학습할 때마다 W를 저장하라\n",
    "\n",
    "# 최종 평가\n",
    "print(model.evaluate(norm_test_x_data, test_t_data)) # loss: 0.4814 - accuracy: 0.8759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ead5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ef15c77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? ㅛ\n",
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "394/394 [==============================] - 0s 709us/step - loss: 2.3833 - accuracy: 0.0706\n",
      "[2.383298873901367, 0.07055555284023285]\n"
     ]
    }
   ],
   "source": [
    "# 저장한 model 불러서 다시 사용하기\n",
    "# 1. 학습하지 않은 상태로 evaluation 진행 -> 좋지 않은 결과\n",
    "# 2. checkpoint에 있는 W를 load한 후 model을 재설정하고 평가 -> 좋은 결과\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense # Flatten는 input layer, Dense는 output layer용\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "\n",
    "# 기존에는 test_x_data, test_t_data 이 두 데이터를 validation 용도로 사용함. 이제는 test 용도로 사용할 것\n",
    "# Keras는 학습할 때 train data를 일정 부분 나누어 자체 평가가 가능\n",
    "\n",
    "# Data Split\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)\n",
    "\n",
    "# loss를 sparse_categorical_crossentropy로 지정할 것이기 때문에 one-hot endoding 처리가 필요 없음\n",
    "\n",
    "# model 생성\n",
    "model = Sequential()\n",
    "\n",
    "# layer 추가\n",
    "model.add(Flatten(input_shape=(norm_train_x_data.shape[1],))) # input layer. 튜플로 입력\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 학습 없이 최종 평가\n",
    "print(model.evaluate(norm_test_x_data, test_t_data)) # accuracy: 0.0706. 정확도 7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eac21f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 0s 651us/step - loss: 0.4814 - accuracy: 0.8759\n",
      "[0.4814387261867523, 0.8758730292320251]\n"
     ]
    }
   ],
   "source": [
    "# 2. checkpoint에 있는 W를 load한 후 model을 재설정하고 평가 -> 좋은 결과\n",
    "\n",
    "checkpoint_path = './training_ckpt/cp.ckpt'\n",
    "model.load_weights(checkpoint_path)\n",
    "print(model.evaluate(norm_test_x_data, test_t_data)) # accuracy: 0.8759. 정확도 87%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_TF2] *",
   "language": "python",
   "name": "conda-env-machine_TF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
